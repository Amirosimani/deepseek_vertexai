{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0wWJBtIqZ07znKl6oe3dz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amirosimani/deepseek_vertexai/blob/main/deepseek_on_vertex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n"
      ],
      "metadata": {
        "id": "YJPLxIdz2WXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "|||\n",
        "|----------|-------------|\n",
        "| Author(s)   | amirimani@ |\n",
        "| Last updated | 10/02/2025 |\n",
        "<br><br>\n"
      ],
      "metadata": {
        "id": "boZaMQp4J2kU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook showcases how to deploy DeepSeek R1 Distill Qwen 7B from the Hugging Face Hub on Vertex AI using Vertex AI Model Garden. It also shows how to prototype and deploy a ReAct agent with google search tool using Langchain."
      ],
      "metadata": {
        "id": "F30k-9TMjknR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VPjg57NnjkD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Vertex AI SDK and other required packages\n"
      ],
      "metadata": {
        "id": "bLvoFhxwkNaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --quiet google-cloud-aiplatform\n",
        "# !pip install --quiet langchain langchain_community\n",
        "# !pip install --quiet langchain_google_genai langchain_google_community\n",
        "# !pip install --quiet tiktoken"
      ],
      "metadata": {
        "id": "_In1NIeyqmD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Restart runtime\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fePFR5LskP5l",
        "outputId": "743454ca-953a-4e21-c18e-79e888dff519"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Authenticate your notebook environment (Colab only)"
      ],
      "metadata": {
        "id": "JpFcqlH9kWv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ],
      "metadata": {
        "id": "1eywiE3RkZjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import vertexai\n",
        "from huggingface_hub import get_token\n",
        "\n",
        "from google.cloud import aiplatform\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "sa1WbMlxjRT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = # YOUR PROJECT NAME\n",
        "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")\n",
        "\n",
        "BUCKET_NAME = \"deepseek-amir\"\n",
        "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION, staging_bucket=BUCKET_URI)\n",
        "! gsutil mb -p $PROJECT_ID -l $LOCATION $BUCKET_URI\n"
      ],
      "metadata": {
        "id": "dhmpTPHVk9od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set the model ID from Hugging Face Hub. In this case, you use DeepSeek-R1-Distill-Qwen-7B, a dense model distilled from DeepSeek-R1 good at math."
      ],
      "metadata": {
        "id": "ER2c8ZZzlSdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_ID = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\""
      ],
      "metadata": {
        "id": "goykqeMnlJuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Register and Deploy DeepSeek model on Vertex AI\n",
        "\n",
        "Please note that you only have to register and deploy the model to vertex only once"
      ],
      "metadata": {
        "id": "D7f9yXUjlVbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deepseek_model = aiplatform.Model.upload(\n",
        "    display_name=MODEL_ID.replace(\"/\", \"--\").lower(),\n",
        "    serving_container_image_uri=\"us-docker.pkg.dev/deeplearning-platform-release/vertex-model-garden/vllm-inference.cu121.0-6.ubuntu2204.py310\",\n",
        "    serving_container_args=[\n",
        "        \"python\",\n",
        "        \"-m\",\n",
        "        \"vllm.entrypoints.api_server\",\n",
        "        \"--host=0.0.0.0\",\n",
        "        \"--port=8080\",\n",
        "        f\"--model={MODEL_ID}\",\n",
        "        \"--tensor-parallel-size=1\",\n",
        "        \"--max-model-len=16384\",\n",
        "        \"--enforce-eager\",\n",
        "    ],\n",
        "    serving_container_ports=[8080],\n",
        "    serving_container_predict_route=\"/generate\",\n",
        "    serving_container_health_route=\"/ping\",\n",
        "    serving_container_environment_variables={\n",
        "        \"HF_TOKEN\": get_token(),\n",
        "        \"DEPLOY_SOURCE\": \"notebook\",\n",
        "    },\n",
        ")\n",
        "deepseek_model.wait()\n"
      ],
      "metadata": {
        "id": "Dt4zS7-ykuXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the model is registered on Vertex AI, you can deploy the model to an endpoint. This can take around 20 minutes.\n",
        "\n"
      ],
      "metadata": {
        "id": "o17-2wkulr2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deepseek_endpoint = aiplatform.Endpoint.create(\n",
        "    display_name=MODEL_ID.replace(\"/\", \"--\").lower() + \"-endpoint\"\n",
        ")\n",
        "\n",
        "deployed_deepseek_model = deepseek_model.deploy(\n",
        "    endpoint=deepseek_endpoint,\n",
        "    machine_type=\"g2-standard-12\",\n",
        "    accelerator_type=\"NVIDIA_L4\",\n",
        "    accelerator_count=1,\n",
        "    sync=False,\n",
        ")"
      ],
      "metadata": {
        "id": "7yXUkVrMlYJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate predictions using Deepseek\n",
        "\n",
        "either get the endpoint name from the console or use aiplatform.Endpoint.list() to see them here."
      ],
      "metadata": {
        "id": "6NLrRcuulv1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "endpoint = aiplatform.Endpoint(endpoint_name=\"YOUR ENDPOINT NAME. something like 5128549095566770688\")"
      ],
      "metadata": {
        "id": "l6aBF1IhlgiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_request = {\n",
        "    \"instances\": [\n",
        "        {\n",
        "            \"@requestFormat\": \"textGeneration\",\n",
        "            \"prompt\":\"Is Hawaiian cuisine vegan friendly?\",\n",
        "            \"max_tokens\": 2048,\n",
        "            \"temperature\": 0.7,\n",
        "        }\n",
        "    ]\n",
        "}"
      ],
      "metadata": {
        "id": "whAcL0sJmGdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = endpoint.predict(instances=prediction_request[\"instances\"])\n",
        "for prediction in output.predictions[0]:\n",
        "    print(\"------- DeepSeek prediction -------\")\n",
        "    print(prediction[\"message\"][\"content\"])\n",
        "    print(\"---------------------------------\\n\")"
      ],
      "metadata": {
        "id": "FJt5N-TWmLKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ReAct Agent with tool calling using DeepSeek"
      ],
      "metadata": {
        "id": "nCqmmtZ-mNfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ],
      "metadata": {
        "id": "MRqm03Sz5_fF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from pydantic import Field\n",
        "from typing import List, Dict, Any, Optional, Union\n",
        "\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "from langchain import hub\n",
        "from langchain.llms.base import LLM\n",
        "from langchain.chat_models.base import BaseChatModel\n",
        "from langchain.schema import LLMResult, AIMessage\n",
        "from langchain.agents import AgentExecutor, Tool, create_react_agent\n",
        "from langchain_community.utilities import GoogleSearchAPIWrapper\n",
        "from langchain.schema import AgentAction, AgentFinish, LLMResult\n",
        "from langchain.callbacks.base import BaseCallbackHandler"
      ],
      "metadata": {
        "id": "hNB6tdiEWG62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "MAX_ITERATIONS = 5\n",
        "MAX_EXECUTION_TIME = 45\n",
        "\n",
        "GENERATION_CONFIG = {\n",
        "    \"temperature\": 0.8,\n",
        "    \"max_tokens\": 2048,\n",
        "}\n",
        "\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE-API')\n",
        "CSE_ID = userdata.get(\"CSE-ID\")"
      ],
      "metadata": {
        "id": "R0N7uwrRXOQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Vertex AI client\n",
        "PROJECT_ID = \"amir-genai-bb\"\n",
        "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")\n",
        "aiplatform.init(project=PROJECT_ID, location=LOCATION)\n",
        "endpoint = aiplatform.Endpoint(endpoint_name=\"YOUR ENDPOINT NAME. something like 5128549095566770688\")"
      ],
      "metadata": {
        "id": "N-W0pTxRBFwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code implements a ReAct agent executor designed to interact with a custom large language model (LLM) and external tools, specifically Google Search. The `CustomLLM`class acts as a bridge, adapting a Vertex AI LLM to the LangChain framework. The `ReActAgentExecutor` class orchestrates the agent's behavior, setting up the LLM, search tools, and the ReAct agent itself. It allows users to provide an input query, which the agent processes by strategically using the search tool and the LLM to generate a response. Additionally, a `TokenCountingCallbackHandler` is incorporated to track the token usage during the agent's execution, facilitating cost analysis and performance monitoring."
      ],
      "metadata": {
        "id": "XFTpyr1PCcIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomLLM(LLM):\n",
        "    \"\"\"A wrapper for the custom Vertex AI LLM that conforms to LangChain's LLM API.\"\"\"\n",
        "\n",
        "    model_name: str = Field(default=\"vertex-ai\")  # Renamed `model` to `model_name` to avoid conflicts\n",
        "    generation_config: Dict[str, Any] = Field(default_factory=dict)\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"custom-vertex-ai\"\n",
        "\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
        "        \"\"\"Generate text using the Vertex AI LLM.\"\"\"\n",
        "        prediction_request = {\n",
        "            \"instances\": [\n",
        "                {\n",
        "                    \"@requestFormat\": \"textGeneration\",\n",
        "                    \"prompt\": prompt,\n",
        "                    \"max_tokens\": self.generation_config.get(\"max_tokens\", 2048),\n",
        "                    \"temperature\": self.generation_config.get(\"temperature\", 0.8),\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        response = endpoint.predict(instances=prediction_request[\"instances\"])\n",
        "        return response.predictions[0] if response.predictions else \"\"\n",
        "\n",
        "class ReActAgentExecutor:\n",
        "    \"\"\"\n",
        "    A class to run the ReAct agent with specified configurations and tools.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: str,\n",
        "        generation_config: Dict,\n",
        "        max_iterations: int,\n",
        "        max_execution_time: int,\n",
        "        google_api_key: str=GOOGLE_API_KEY,\n",
        "        cse_id: str=CSE_ID,\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.generation_config = generation_config\n",
        "        self.max_iterations = max_iterations\n",
        "        self.max_execution_time = max_execution_time\n",
        "        self.google_api_key = google_api_key\n",
        "        self.cse_id = cse_id\n",
        "        self.llm = None\n",
        "        self.tools = None\n",
        "        self.agent = None\n",
        "        self.agent_executor = None\n",
        "        self.token_callback = None\n",
        "\n",
        "        self._setup_llm()\n",
        "        self._setup_tools()\n",
        "        self._setup_agent()\n",
        "\n",
        "    def _setup_llm(self):\n",
        "        \"\"\"Initializes the custom LLM.\"\"\"\n",
        "        self.llm = CustomLLM(model=self.model, generation_config=self.generation_config)\n",
        "\n",
        "    def _setup_tools(self):\n",
        "        \"\"\"Sets up the tools for the agent.\"\"\"\n",
        "        search = GoogleSearchAPIWrapper(\n",
        "            google_api_key=self.google_api_key, google_cse_id=self.cse_id\n",
        "        )\n",
        "        self.tools = [\n",
        "            Tool(\n",
        "                name=\"Google Search\",\n",
        "                func=search.run,\n",
        "                description=\"Useful for finding information on current events, comparisons, or diverse perspectives.\",\n",
        "            ),\n",
        "        ]\n",
        "\n",
        "    def _setup_agent(self):\n",
        "        \"\"\"Sets up the ReAct agent and executor.\"\"\"\n",
        "        prompt = hub.pull(\"hwchase17/react\")\n",
        "        system_instruction = \"Once you are done finding the answer, only return Yes or No\"\n",
        "        prompt.template = system_instruction + \"\\n\" + prompt.template\n",
        "\n",
        "        self.agent = create_react_agent(self.llm, self.tools, prompt)\n",
        "        self.token_callback = TokenCountingCallbackHandler(self.model)\n",
        "        self.agent_executor = AgentExecutor(\n",
        "            agent=self.agent,\n",
        "            tools=self.tools,\n",
        "            verbose=False,\n",
        "            handle_parsing_errors=True,\n",
        "            max_iterations=self.max_iterations,\n",
        "            max_execution_time=self.max_execution_time,\n",
        "            callbacks=[self.token_callback],\n",
        "        )\n",
        "\n",
        "    def run(self, input_data: Union[Dict, str]) -> Dict:\n",
        "        \"\"\"\n",
        "        Runs the agent with the given input data.\n",
        "        \"\"\"\n",
        "        if isinstance(input_data, str):\n",
        "            input_data = {\"input\": input_data}\n",
        "\n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            result = self.agent_executor.invoke(input_data)\n",
        "            result[\"total_token\"] = self.token_callback.total_token\n",
        "            self.token_callback.reset()\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "            result = {\"error\": str(e)}\n",
        "        end_time = time.time()\n",
        "        result[\"wall_time\"] = end_time - start_time\n",
        "\n",
        "        return result\n",
        "\n",
        "class TokenCountingCallbackHandler(BaseCallbackHandler):\n",
        "    \"\"\"Callback handler for counting tokens used by the language model.\"\"\"\n",
        "    def __init__(self, model_name: str):\n",
        "        self.model_name = model_name\n",
        "        self.total_token = 0\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the counters for the next chain run.\"\"\"\n",
        "        self.total_token = 0\n"
      ],
      "metadata": {
        "id": "595XAIrTSvu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = CustomLLM(\n",
        "    model=\"vertex-ai\",\n",
        "    generation_config=GENERATION_CONFIG\n",
        ")\n",
        "\n",
        "agent_executor = ReActAgentExecutor(\n",
        "    model=llm,\n",
        "    generation_config=GENERATION_CONFIG,\n",
        "    max_iterations=8,\n",
        "    max_execution_time=60,\n",
        "    google_api_key=GOOGLE_API_KEY,\n",
        "    cse_id=CSE_ID\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gB9r0JnuXdo1",
        "outputId": "efed6e48-7d30-4bc8-e54f-56c0f1440272"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = agent_executor.run(train_ds[10][\"question\"])"
      ],
      "metadata": {
        "id": "XnEBhUxDg_7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCg72jiyhNmg",
        "outputId": "4fda7d48-8294-4383-d684-a3304aaae4f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Does a Starbucks passion tea have ginger in it?',\n",
              " 'output': 'Agent stopped due to iteration limit or time limit.',\n",
              " 'total_token': 0,\n",
              " 'wall_time': 147.72699403762817}"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    }
  ]
}